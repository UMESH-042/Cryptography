{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1"
      ],
      "metadata": {
        "id": "ZRcV_4p5w5By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "# Load the car dataset from CSV\n",
        "url = \"car.csv\"\n",
        "df = pd.read_csv(url, header=None)\n",
        "# Extract column labels from the first row\n",
        "columns = df.iloc[0].tolist()\n",
        "df = df[1:] # Skip the first row with column labels\n",
        "df.columns = columns # Set the column names\n",
        "# Convert categorical variables to numerical using one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=[\"buying\", \"maint\", \"doors\",\n",
        "\"person\", \"lug_boot\", \"safety\"])\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df_encoded.drop(\"class_variable\", axis=1)\n",
        "y = df_encoded[\"class_variable\"]\n",
        "# Split the data into training (60%) and testing (40%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
        "stratify=y)\n",
        "# Create a Decision Tree classifier with entropy as the criterion and\n",
        "max_depth=None\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42,\n",
        "max_depth=None)\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "# Evaluate the performance using confusion matrix and F1-score\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "# Print the confusion matrix and F1 score\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "Kyg4Ey1nw6Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2\n"
      ],
      "metadata": {
        "id": "3E03dveyw9TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mport pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import numpy as np\n",
        "# Load the car dataset from CSV\n",
        "url = \"car.csv\"\n",
        "df = pd.read_csv(url, header=None)\n",
        "# Extract column labels from the first row\n",
        "columns = df.iloc[0].tolist()\n",
        "df = df[1:] # Skip the first row with column labels\n",
        "df.columns = columns # Set the column names\n",
        "# Convert categorical variables to numerical using one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=[\"buying\", \"maint\", \"doors\", \"person\",\n",
        "\"lug_boot\", \"safety\"])\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df_encoded.drop(\"class_variable\", axis=1)\n",
        "y = df_encoded[\"class_variable\"]\n",
        "# Number of repetitions\n",
        "num_repeats = 20\n",
        "# Lists to store results\n",
        "f1_scores = []\n",
        "conf_matrices = []\n",
        "# Define a range of max_depth values to test\n",
        "max_depth_values = [3, 5, 7, 10, 15, 20,25, None]\n",
        "for max_depth in max_depth_values:\n",
        "f1_scores_iter = []\n",
        "conf_matrices_iter = []\n",
        "for _ in range(num_repeats):\n",
        "# Split the data into training (60%) and testing (40%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.4, stratify=y)\n",
        "# Create a Decision Tree classifier with entropy as the criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42,\n",
        "max_depth=max_depth)\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "# Evaluate the performance using confusion matrix and F1-score\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "# Store results\n",
        "conf_matrices_iter.append(conf_matrix)\n",
        "f1_scores_iter.append(f1)\n",
        "# Calculate and print the average F1-score and confusion matrix for the\n",
        "current max_depth\n",
        "average_f1_iter = np.mean(f1_scores_iter, axis=0)\n",
        "average_conf_matrix_iter = np.mean(conf_matrices_iter, axis=0)\n",
        "print(f\"\\nMax Depth: {max_depth}\")\n",
        "print(\"Average Confusion Matrix:\")\n",
        "print(average_conf_matrix_iter.astype(int))\n",
        "print(\"Average F1 Score:\", average_f1_iter)\n",
        "# Store results for all max_depth values\n",
        "f1_scores.append(average_f1_iter)\n",
        "conf_matrices.append(average_conf_matrix_iter)\n",
        "# Find the max_depth that gives the highest average F1-score\n",
        "best_max_depth_index = np.argmax(f1_scores)\n",
        "best_max_depth = max_depth_values[best_max_depth_index]\n",
        "print(\"\\nBest Max Depth:\", best_max_depth)"
      ],
      "metadata": {
        "id": "SnSuumfHw-db"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}